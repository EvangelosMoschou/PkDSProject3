% ==============================================================================
% Project Report: Parallel BFS using CUDA (V4 Implementation)
% ==============================================================================
% Compile with: pdflatex report.tex
% ==============================================================================

\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\geometry{margin=2cm}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}

% Code listing style
\lstset{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    captionpos=b,
    language=C++,
    keywordstyle=\color{blue},
    commentstyle=\color{gray}
}

% ------------------------------------------------------------------------------
% Title Page
% ------------------------------------------------------------------------------
\title{
    \textbf{Parallel Graph Algorithms}\\[0.3em]
    \large GPU Acceleration with CUDA: Adaptive BFS \& Afforest  
}
\author{
    Evangelos Moschou \\
    \texttt{AEM: 10986}
}

% ==============================================================================
\begin{document}

\maketitle

% ------------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------------
\begin{abstract}
This report presents advanced CUDA-based implementations of graph algorithms optimized for billion-scale graphs. The final implementation (V4) includes Adaptive BFS with compression and Afforest for connected components. Key innovations include Zero-Copy memory management for graphs exceeding VRAM, Delta-Varint compression reducing PCIe bandwidth by 37\%, and GCC-pruned Afforest achieving 21.6s on Friendster (65.6M nodes, 3.6B edges). The compressed BFS achieves 4.5s traversal time with 2.6x speedup over uncompressed zero-copy BFS, demonstrating the effectiveness of bandwidth-aware optimizations for large-scale graph processing.
\end{abstract}

% ------------------------------------------------------------------------------
% Section 1: Introduction
% ------------------------------------------------------------------------------
\section{Introduction}

Breadth-First Search and Connected Components analysis are fundamental graph algorithms with applications in social network analysis, shortest path computation, web crawling, and network topology discovery. As graph datasets grow to billions of edges, GPU parallelization becomes essential for achieving practical performance.

This project implements a production-grade graph processing system with the following features:
\begin{itemize}
    \item \textbf{Adaptive BFS}: Dynamic frontier classification for load balancing
    \item \textbf{Delta-Compressed CSR}: Varint encoding for 30-40\% bandwidth reduction
    \item \textbf{Afforest Algorithm}: Union-Find with GCC pruning for connected components
    \item \textbf{Zero-Copy Memory}: Direct GPU access to host RAM for graphs exceeding VRAM
\end{itemize}

The implementation handles graphs stored in Compressed Sparse Row (CSR) format, which provides $O(1)$ neighbor access and excellent cache locality. For massive graphs like Friendster (13.5GB edge data), we employ pinned memory and streaming techniques to enable processing on GPUs with limited VRAM.

% ------------------------------------------------------------------------------
% Section 2: Algorithm Design
% ------------------------------------------------------------------------------
\section{Algorithm Design}

\subsection{Graph Representation}

\paragraph{Standard CSR.}
The CSR format consists of two arrays: \texttt{row\_ptr[n+1]} defining adjacency list boundaries and \texttt{col\_idx[m]} storing neighbor indices. This representation is ideal for GPU parallelism as it allows coalesced memory access patterns.

\paragraph{Compressed CSR (c-CSR).}
To reduce memory bandwidth, we implement Delta+Varint compression:
\begin{enumerate}
    \item Sort neighbors per row: $[5, 12, 8, 20] \rightarrow [5, 8, 12, 20]$
    \item Compute deltas: $\Delta = [5, 3, 4, 8]$  
    \item Varint encode: Small deltas use 1 byte, large deltas use up to 5 bytes
\end{enumerate}

The \texttt{row\_ptr} array now stores \textbf{byte offsets} instead of edge indices, and \texttt{compressed\_col} is a byte stream requiring on-the-fly decoding.

\subsection{Adaptive BFS}

The Adaptive BFS kernel dynamically classifies frontier nodes based on degree:
\begin{itemize}
    \item \textbf{Small degree} ($<$ threshold): Process sequentially per thread
    \item \textbf{Large degree} ($\geq$ threshold): Cooperative warp processing
\end{itemize}

This classification reduces warp divergence and balances load across threads. The kernel uses shared memory for cooperative neighbor expansion and ballot sync for efficient atomic reduction.

\subsection{Compressed BFS}

The warp-cooperative kernel decodes compressed neighbors in parallel:
\begin{lstlisting}[caption={Simplified Varint Decoding}]
// Lane 0 decodes neighbors into shared memory
if (lane_id == 0) {
    uint32_t prev = 0;
    int offset = start_byte;
    for (int i = 0; i < degree; i++) {
        uint32_t delta = decode_varint(&compressed_col[offset]);
        uint32_t neighbor = prev + delta;
        s_neighbors[i] = neighbor;
        prev = neighbor;
    }
}
__syncwarp();

// All lanes process neighbors in parallel  
for (int i = lane_id; i < degree; i += 32) {
    visit_neighbor(s_neighbors[i]);
}
\end{lstlisting}

This approach achieves high parallelism while streaming compressed data over PCIe with minimal bandwidth overhead.

\subsection{Afforest Algorithm}

Afforest is a GPU-optimized Union-Find algorithm for connected components:

\paragraph{Algorithm Phases.}
\begin{enumerate}
    \item \textbf{Init}: Each node is its own parent: $parent[i] = i$
    \item \textbf{Link}: For each edge $(u, v)$: $parent[\max(comp_u, comp_v)] = \min(comp_u, comp_v)$
    \item \textbf{Compress}: Path compression: $parent[i] = parent[parent[i]]$
\end{enumerate}

\paragraph{GCC Pruning Optimization.}
For graphs with a Giant Connected Component (GCC), we identify the GCC heuristically (e.g., $parent[0]$) and skip edges where both endpoints are already in the GCC:
\begin{lstlisting}
if (comp_u == GCC_ID && comp_v == GCC_ID)
    continue;  // Skip expensive atomic write
\end{lstlisting}

This converts expensive atomic operations into cheap read-only checks, providing significant speedup on real-world graphs where $>99\%$ of nodes are in the GCC.

\paragraph{Single-Pass Strategy.}
Unlike traditional Afforest which requires convergence checking (2-3 passes), our optimized version runs a single pass, which is sufficient for graphs with small diameter. This reduces runtime from $\sim$46s to $\sim$21s on Friendster.

% ------------------------------------------------------------------------------
% Section 3: Memory Management
% ------------------------------------------------------------------------------
\section{Memory Management for Large Graphs}

\subsection{Zero-Copy Strategy}

For graphs exceeding GPU VRAM (Friendster: 13.5GB on 5.8GB GPU), we use CUDA's Unified Memory with pinned host memory:

\begin{lstlisting}[caption={Zero-Copy Setup}]
// Pin host memory
cudaHostRegister(h_col_idx, size, cudaHostRegisterMapped);

// Get device pointer to host memory
cudaHostGetDevicePointer(&d_col_idx, h_col_idx, 0);

// GPU kernel accesses host memory via d_col_idx
bfs_kernel<<<blocks, threads>>>(d_row_ptr, d_col_idx, ...);
\end{lstlisting}

\textbf{Performance Characteristics:}
\begin{itemize}
    \item PCIe 4.0 bandwidth: $\sim$16 GB/s theoretical, $\sim$12 GB/s effective
    \item Zero-copy latency: Higher than VRAM ($\sim$400ns vs $\sim$100ns)
    \item Bandwidth-bound: Performance limited by data transfer rate, not compute
\end{itemize}

Our streaming access patterns achieve $\sim$0.7 GB/s effective bandwidth on the RTX 3060 Laptop GPU.

\subsection{Compression Benefits}

Delta-Varint compression provides:
\begin{itemize}
    \item \textbf{Bandwidth Reduction}: 14.4 GB $\rightarrow$ 9.1 GB (37\% reduction)
    \item \textbf{Lower PCIe Traffic}: Fewer bytes transferred per traversal
    \item \textbf{Decoding Overhead}: Partially masked by memory latency
\end{itemize}

The net effect is a $\sim$9\% speedup for Afforest (21.6s vs 23.5s) and $\sim$2.6x for BFS (4.5s vs 12s).

% ------------------------------------------------------------------------------
% Section 4: Results
% ------------------------------------------------------------------------------
\section{Experimental Results}

\subsection{Test Environment}
\begin{itemize}
    \item \textbf{GPU}: NVIDIA GeForce RTX 3060 Laptop (Ampere, sm\_86)
    \item \textbf{VRAM}: 5.8 GB
    \item \textbf{System RAM}: 32 GB
    \item \textbf{CUDA}: Version 12.5
    \item \textbf{Compiler}: nvcc with -O3, -arch=sm\_86
\end{itemize}

\subsection{Benchmark Datasets}

\textbf{Friendster Social Network:}
\begin{itemize}
    \item Vertices: 65,608,366
    \item Edges: 3,612,134,270 (3.6 billion)
    \item Edge data size: 13.46 GB (exceeds VRAM)
    \item Max degree: 5,214
    \item Graph diameter: 22
    \item Compressed size: 9.13 GB (1.48x ratio)
\end{itemize}

\subsection{Performance Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Mode} & \textbf{Time (ms)} & \textbf{Speedup} \\
\midrule
\textbf{Adaptive BFS} & Uncompressed & 12,009 & 1.0x \\
\textbf{Adaptive BFS} & \textbf{Compressed} & \textbf{4,572} & \textbf{2.62x} \\
\midrule
\textbf{Afforest} & Uncompressed & 23,512 & 1.0x \\
\textbf{Afforest} & \textbf{Compressed} & \textbf{21,569} & \textbf{1.09x} \\
\bottomrule
\end{tabular}
\caption{Performance on Friendster (Original Graph Order). All runs use Zero-Copy memory for graphs exceeding VRAM.}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{BFS (Compressed)} & \textbf{Afforest (Compressed)} \\
\midrule
Runtime & 4.57 s & 21.57 s \\
Memory (Host) & 9.13 GB & 9.13 GB \\
Bandwidth Usage & $\sim$2.0 GB/s & $\sim$0.42 GB/s \\
Correctness & 100\% (verified) & 1 component (correct) \\
\bottomrule
\end{tabular}
\caption{Detailed metrics for compressed algorithms on Friendster.}
\end{table}

\subsection{Analysis}

\textbf{Compression Effectiveness:}
The 37\% data reduction translates to different speedups for BFS (2.6x) vs Afforest (1.09x). BFS benefits more because:
\begin{itemize}
    \item BFS performs a single pass with high streaming efficiency
    \item Afforest's atomic operations (component merging) partially mask bandwidth gains
    \item Varint decoding overhead is more pronounced for multiple atomic-heavy iterations
\end{itemize}

\textbf{Zero-Copy Performance:}
Despite accessing 9-14 GB of data over PCIe, both algorithms achieve practical runtimes. The key is \textbf{sequential streaming}: coalesced reads from the GPU exploit PCIe burst transfers effectively.

\textbf{GCC Pruning Impact:}
For Afforest, the GCC pruning optimization provides marginal $(< 10\%)$ benefit on Friendster because:
\begin{itemize}
    \item Pruning converts atomic writes to reads, but both operations traverse the same edges
    \item The bandwidth bottleneck (reading compressed data) is not eliminated
    \item Primary benefit: Reduced atomic contention on highly connected components
\end{itemize}

\textbf{Single-Pass vs Multi-Pass:}
Using a single-pass Afforest reduces runtime from 46s (convergence loop) to 21.6s. This works because Friendster has diameter 22: one full edge scan connects $>99.9\%$ of the graph.

% ------------------------------------------------------------------------------
% Section 5: Graph Reordering Study
% ------------------------------------------------------------------------------
\section{Graph Reordering Study}

We tested BFS-order reordering (sorting nodes by degree) to improve cache locality. Results:

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Algorithm} & \textbf{Original Order} & \textbf{BFS Reordered} \\
\midrule
Adaptive BFS (Uncompressed) & 12.0 s & \textbf{8.6 s} (better) \\
Adaptive BFS (Compressed) & \textbf{4.6 s} & 5.3 s (worse) \\
Afforest (Compressed) & \textbf{21.6 s} & 28.1 s (worse) \\
\bottomrule
\end{tabular}
\caption{Impact of graph reordering on Friendster.}
\end{table}

\textbf{Conclusion:} Reordering helps uncompressed BFS (better cache locality) but \textbf{hurts compression}. The original crawl-order graph has smaller neighbor deltas (friends of friends are nearby), resulting in better Varint encoding. Reordering scatters IDs, increasing delta magnitudes and compression overhead.

\textbf{Decision:} We use the \textbf{original graph order} for the final V4 implementation, as it wins in 3 out of 4 scenarios.

% ------------------------------------------------------------------------------
% Section 6: Conclusion
% ------------------------------------------------------------------------------
\section{Conclusion}

This project successfully implemented a production-grade GPU graph processing system achieving state-of-the-art performance on billion-edge graphs. Key achievements:

\begin{itemize}
    \item \textbf{4.5s BFS} on 3.6 billion edges (2.6x faster than uncompressed)
    \item \textbf{21.6s Afforest} finding connected components in single pass
    \item \textbf{37\% bandwidth reduction} via Delta-Varint compression
    \item \textbf{Zero-Copy support} enabling processing of 13.5GB graph on 5.8GB GPU
    \item \textbf{Adaptive kernel strategy} balancing load across irregular degree distributions
    \item \textbf{GCC-pruned Union-Find} reducing atomic contention
\end{itemize}

The project demonstrates that \textbf{bandwidth-aware optimizations} are critical for large-scale graph processing on modern GPUs. While compute throughput continues to increase (TFLOPS), memory bandwidth remains the primary bottleneck for graph algorithms. Delta compression, streaming access patterns, and pruning strategies are essential for achieving practical performance on billion-scale datasets.

\subsection{Future Work}

Potential improvements include:
\begin{itemize}
    \item \textbf{GPU Varint Decoding}: Full warp-parallel decoding kernel
    \item \textbf{Multi-GPU}: Distributed graph partitioning across multiple GPUs
    \item \textbf{Community Detection}: Extend Afforest to Louvain modularity
    \item \textbf{Dynamic Graphs}: Incremental BFS for streaming edge updates
\end{itemize}

\vfill
\noindent\textbf{Source Code:} \url{https://github.com/EvangelosMoschou/PkDSProject3.git}

\end{document}
