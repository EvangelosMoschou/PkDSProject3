% ==============================================================================
% Project Report: Parallel BFS using CUDA (V4 Implementation)
% ==============================================================================
% Compile with: pdflatex report.tex
% ==============================================================================

\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\geometry{left=1.5cm,right=1.5cm,top=2cm,bottom=2cm}
\usepackage{enumitem}
\setlist{nosep}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}

% ------------------------------------------------------------------------------
% Title Page
% ------------------------------------------------------------------------------
\title{
    \textbf{Parallel Graph Algorithms}\\[0.3em]
    \large GPU Acceleration with CUDA: Adaptive BFS \& Afforest  
}
\author{
    Evangelos Moschou \\
    \texttt{AEM: 10986}
}
\date{}

% ==============================================================================
\begin{document}

\maketitle

% ------------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------------
\begin{abstract}
This report presents a comprehensive study of graph preprocessing techniques to improve performance of GPU-accelerated graph algorithms on billion-scale graphs. The primary focus is the evolution from standard Reverse Cuthill-McKee (RCM) reordering to a novel \textbf{Gap-Aware BFS} approach, which uniquely optimizes for both cache locality and delta-compression efficiency. We evaluate these techniques alongside Delta-Varint compression, Zero-Copy memory management, and adaptive kernels. Experimental results on Friendster (65.6M nodes, 3.6B edges) demonstrate that while traditional RCM degrades compression performance by 15-30\%, our Gap-Aware BFS achieves a \textbf{9.2\% speedup over the optimized baseline} (6.33s vs 6.91s) by preserving community structure during renumbering. This study demonstrates that preprocessing can resolve the antagonism between data locality and compression through careful design.
\end{abstract}

% ------------------------------------------------------------------------------
% Section 1: Introduction
% ------------------------------------------------------------------------------
\section{Introduction}

Graph preprocessing is a critical but often overlooked component of high-performance graph analytics. The physical layout of vertices and edges in memory significantly affects cache utilization, TLB efficiency, and memory bandwidth requirements---key factors that determine whether algorithms are compute-bound or memory-bound on modern GPUs.

\textbf{This project's primary contribution is a systematic study of graph preprocessing techniques, with emphasis on the evolution from RCM to Gap-Aware BFS, and their impact on GPU-accelerated graph algorithms.} While we implement two fundamental algorithms (BFS and Afforest) as case studies, the main objective is to demonstrate that \textbf{preprocessing choices are algorithm-dependent} and must be co-designed with runtime optimizations.

We evaluate several preprocessing and optimization strategies:
\begin{itemize}
    \item \textbf{Gap-Aware BFS}: A novel reordering strategy that sorts neighbors by original ID during layout construction to preserve compression-friendly deltas.
    \item \textbf{RCM Reordering}: Bandwidth minimization for improved spatial locality in uncompressed data.
    \item \textbf{Delta-Compressed CSR}: Varint encoding reducing data transfer by 37\%
    \item \textbf{Zero-Copy Memory}: Streaming access to graphs exceeding VRAM
    \item \textbf{Adaptive Kernels}: Dynamic load balancing for irregular graphs
\end{itemize}

Our experimental results reveal a key insight: \textbf{Standard RCM and compression are antagonistic}, but \textbf{Gap-Aware BFS resolves this trade-off}. While RCM improves cache locality for uncompressed algorithms, it scatters IDs and ruins compression. In contrast, Gap-Aware BFS achieves the spatial locality of RCM while maintaining the small deltas required for efficient Varint encoding.

% ------------------------------------------------------------------------------
% Section 2: Algorithm & Data Structure Design
% ------------------------------------------------------------------------------
\section{Algorithm \& Data Structure Design}

\subsection{Data Representation: Compressed CSR \& Zero-Copy}
Standard CSR allows coalesced GPU access but bottlenecks PCIe bandwidth (~13.5GB for Friendster vs 5.8GB VRAM). We implement **Zero-Copy Streaming** combined with **Delta-Varint Compression**:
\begin{itemize}
    \item \textbf{Delta+Varint}: Neighbors are sorted, and differences ($\Delta$) are stored using variable-length integers (1-5 bytes). This reduces Friendster from 14.4GB to 9.1GB (37\% reduction).
    \item \textbf{Streaming}: We use \texttt{cudaHostRegister} to pin host memory, allowing kernels to access data directly over PCIe (effective bw $\sim$1.4GB/s).
    \item \textbf{Decoding}: A warp-cooperative kernel decodes byte-streams in shared memory. Lane 0 decodes sequentially; all lanes process neighbors in parallel.
\end{itemize}

\subsection{Adaptive BFS \& Afforest}
\textbf{Adaptive BFS} classifies nodes by degree: high-degree nodes use cooperative warp expansion to balance load, while low-degree nodes are processed per-thread. The compressed variant integrates on-the-fly decoding, masking $\sim$400ns PCIe latency with compute.

\textbf{Afforest} (Connected Components) is optimized for single-pass execution on small-diameter graphs. We added a **GCC Pruning** heuristic: before atomic linking, we check if both nodes already belong to the Giant Connected Component (root node 0). heuristic checks reduce atomic contention by 97\% on Friendster, though the algorithm remains bandwidth-bound ($\sim$21s).

% ------------------------------------------------------------------------------
% Section 4: Results
% ------------------------------------------------------------------------------
\section{Experimental Results}

\subsection{Test Environment}
\begin{itemize}
    \item \textbf{GPU}: NVIDIA GeForce RTX 3060 Laptop (Ampere, sm\_86)
    \item \textbf{VRAM}: 5.8 GB
    \item \textbf{System RAM}: 32 GB
    \item \textbf{CUDA}: Version 12.5
    \item \textbf{Compiler}: nvcc with -O3, -arch=sm\_86
\end{itemize}

\subsection{Benchmark Datasets}

\textbf{Friendster Social Network:}
\begin{itemize}
    \item Vertices: 65,608,366
    \item Edges: 3,612,134,270 (3.6 billion)
    \item Edge data size: 13.46 GB (exceeds VRAM)
    \item Max degree: 5,214
    \item Graph diameter: 22
    \item Compressed size: 9.13 GB (1.48x ratio)
\end{itemize}

\subsection{Performance Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Mode} & \textbf{Time (ms)} & \textbf{Effective Speedup} \\
\midrule
\textbf{Adaptive BFS} (Original) & Uncompressed & 12,009 & 1.0x \\
\textbf{Adaptive BFS} (Original) & \textbf{Compressed} & \textbf{6,908} & \textbf{1.74x} \\
\textbf{Adaptive BFS} (Gap-Aware) & \textbf{Compressed} & \textbf{6,334} & \textbf{1.90x} \\
\midrule
\textbf{Afforest} & Uncompressed & 23,512 & 1.0x \\
\textbf{Afforest} & \textbf{Compressed} & \textbf{21,569} & \textbf{1.09x} \\
\bottomrule
\end{tabular}
\caption{Final Performance on Friendster (100\% Reachability). Gap-Aware BFS provides the optimal balance. All runs use Zero-Copy memory.}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{BFS (Gap-Aware Compressed)} & \textbf{Afforest (Compressed)} \\
\midrule
Runtime & 6.33 s & 21.57 s \\
Memory (Host) & 9.13 GB & 9.13 GB \\
Effective Bandwidth & $\sim$1.44 GB/s & $\sim$0.42 GB/s \\
Correctness & 100\% (verified) & 1 component (correct) \\
\bottomrule
\end{tabular}
\caption{Detailed metrics for our best performing configurations on Friendster.}
\end{table}

\subsection{Analysis}
The 37\% data reduction translates to a \textbf{1.9x speedup for BFS} vs 1.09x for Afforest. BFS benefits more due to higher streaming density per frontier layer. Afforest's heavy atomic contention partially masks bandwidth gains, though GCC pruning helps. Both algorithms effectively hide the 400ns PCIe latency through massive thread-level parallelism.

% ------------------------------------------------------------------------------
% Section 5: Graph Preprocessing: RCM and Gap-Aware BFS
% ------------------------------------------------------------------------------
\section{Graph Preprocessing: RCM and Gap-Aware BFS}

\subsection{Motivation and Algorithm}

The Reverse Cuthill-McKee (RCM) algorithm is a graph reordering technique that reduces the \textbf{bandwidth} of the adjacency matrix by renumbering vertices to place connected nodes close together in memory. This improves:
\begin{itemize}
    \item \textbf{Cache locality}: Adjacent vertices in BFS trees map to nearby memory addresses
    \item \textbf{TLB hit rate}: Reduced page working set due to spatial clustering
    \item \textbf{Prefetcher efficiency}: Sequential access patterns enable hardware prefetching
\end{itemize}

\paragraph{RCM Algorithm Overview.}
\begin{enumerate}
    \item Select a peripheral node (low degree, far from graph center) as the starting vertex
    \item Perform BFS traversal, visiting nodes level-by-level
    \item Within each level, sort nodes by \textbf{increasing degree} (Cuthill-McKee)
    \item \textbf{Reverse} the final ordering (Reverse Cuthill-McKee)
\end{enumerate}

The reversal step places high-degree hub nodes at the beginning of the ordering, which empirically improves performance for many graph algorithms by prioritizing well-connected vertices early in traversals.

\subsection{Implementation Details}

Our CUDA implementation uses:
\begin{itemize}
    \item \textbf{Parallel BFS}: GPU-based level-synchronous traversal for fast reordering
    \item \textbf{Radix Sort}: Per-level sorting by degree using CUB library primitives
    \item \textbf{Pseudo-Peripheral Search}: Heuristic to find optimal starting vertex
\end{itemize}

Reordering time for Friendster: $\sim$2.3 seconds for RCM, $\sim$18 minutes for Gap-Aware BFS (due to neighbor sorting).

\subsection{Gap-Aware BFS: Resolving the Locality-Compression Trade-off}

To address the performance degradation seen when combining RCM with compression, we implemented \textbf{Gap-Aware BFS}. The algorithm follows the standard BFS renumbering but introduces a critical modification:
\textbf{During the neighbor enqueue stage, neighbors are sorted by their ORIGINAL ID before being assigned NEW IDs.}

This modification ensures that nodes that were originally part of the same community (and thus had close IDs) are assigned consecutive new IDs. This preserves the "Natural Community Order" benefits for Delta-Varint compression while obtaining the bandwidth reduction benefits of a global BFS ordering.

\subsection{Experimental Results}

We compare four graph orderings on Friendster:
\begin{enumerate}
    \item \textbf{Original Order}: Natural edge insertion order from web crawling.
    \item \textbf{RCM Order}: Bandwidth-minimized via standard RCM.
    \item \textbf{Gap-Aware BFS}: Locality-optimized with original gap preservation.
    \item \textbf{Degree-Sorted}: Baseline reordering (not recommended).
\end{enumerate}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm \& Mode} & \textbf{Original} & \textbf{Gap-Aware BFS} & \textbf{Speedup / Delta} \\
\midrule
BFS Compressed & 6,908 ms & \textbf{6,334 ms} & -8.3\% (Best) \\
BFS Uncompressed & 12,009 ms & 8,812 ms & -26.6\% (Good) \\
RCM (Uncompressed) & 12,009 ms & 8,602 ms & -28.3\% (Peak Locality) \\
\bottomrule
\end{tabular}
\caption{Comparison of Reordering Strategies on Friendster (100\% Reachability). Gap-Aware BFS provides the best balance for compressed data.}
\end{table}

\subsection{Analysis: Resolving the Antagonism}

\paragraph{Locality Gains.}
Like RCM, Gap-Aware BFS provides a global leveling that ensures nodes visited together are stored together. This reduced our compressed runtime from 6.9s in the original crawl order to 6.3s.

\paragraph{Compression Efficiency.}
The "sorting-by-original-ID" trick was the breakthrough. By keeping the delta gaps small ($\Delta \approx 1$ for community members), we avoided the 15-30\% penalty observed with standard RCM. Gap-Aware BFS is the only reordering strategy we evaluated that successfully combines both optimization axes.

\subsection{Design Guidelines}

Based on our results, we recommend:
\begin{itemize}
    \item \textbf{Use Gap-Aware BFS} for: Optimal performance on compressed social graphs. It provides 8-10\% speedup over the crawl order without breaking compression.
    \item \textbf{Use RCM} for: Uncompressed, cache-sensitive algorithms where total bandwidth is not the bottleneck but latency is.
\end{itemize}

\textbf{Key Insight:} There is no "universal" optimal preprocessing. RCM optimizes the \emph{global matrix envelope} (bandwidth), while compression requires \emph{local neighbor proximity} (gaps). Ideally, one should use community-preserving reordering for compression, which remains an open area for future optimization in this system.

% ------------------------------------------------------------------------------
% Section 6: Conclusion
% ------------------------------------------------------------------------------
\section{Conclusion}

This project provides a comprehensive study of \textbf{graph preprocessing and its interaction with GPU optimization strategies}. The central contribution is demonstrating that preprocessing choices (RCM reordering vs. natural ordering) have \textbf{algorithm-dependent} performance impacts that must be considered during system design.

\paragraph{Key Findings.}
\begin{itemize}
    \item \textbf{RCM reordering provides 28\% speedup} for uncompressed BFS but severely degrades delta-compression efficiency.
    \item \textbf{Gap-Aware BFS resolves the locality-compression conflict}, achieving a 9.2\% speedup over the optimized baseline on Friendster.
    \item \textbf{Hierarchical locality preservation} is the key to managing billion-scale graphs on consumer GPUs: small gaps for compression and global leveling for cache efficiency.
\end{itemize}

\paragraph{Broader Impact.}
Beyond the specific algorithms implemented (BFS and Afforest), this work establishes \textbf{design principles for graph preprocessing}:
\begin{enumerate}
    \item Analyze algorithm memory access patterns (random vs. sequential, read-heavy vs. atomic-heavy)
    \item Measure sensitivity to cache locality vs. memory bandwidth
    \item Select preprocessing strategy that aligns with the dominant bottleneck
    \item Consider one-time preprocessing cost vs. repeated execution benefits
\end{enumerate}

Graph preprocessing is not a preprocessing step to be applied blindly---it is a \textbf{co-design problem} requiring deep understanding of both algorithm characteristics and hardware constraints. On memory-limited GPUs processing billion-scale graphs, the choice between RCM reordering and compression-friendly orderings can determine whether an algorithm is practical or infeasible.

Future work should explore adaptive preprocessing that selects orderings dynamically based on runtime profiling, as well as hybrid schemes that apply RCM to subgraphs while preserving global clustering properties.

\vfill
\noindent\textbf{Source Code:} \url{https://github.com/EvangelosMoschou/PkDSProject3.git}

\end{document}
