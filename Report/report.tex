% ==============================================================================
% Project Report: Parallel BFS using CUDA (V4 Implementation)
% ==============================================================================
% Compile with: pdflatex report.tex
% ==============================================================================

\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\geometry{margin=2cm}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}

% ------------------------------------------------------------------------------
% Title Page
% ------------------------------------------------------------------------------
\title{
    \textbf{Parallel Graph Algorithms}\\[0.3em]
    \large GPU Acceleration with CUDA: Adaptive BFS \& Afforest  
}
\author{
    Evangelos Moschou \\
    \texttt{AEM: 10986}
}
\date{}

% ==============================================================================
\begin{document}

\maketitle

% ------------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------------
\begin{abstract}
This report presents a comprehensive study of graph preprocessing techniques to improve performance of GPU-accelerated graph algorithms on billion-scale graphs. The primary focus is the evolution from standard Reverse Cuthill-McKee (RCM) reordering to a novel \textbf{Gap-Aware BFS} approach, which uniquely optimizes for both cache locality and delta-compression efficiency. We evaluate these techniques alongside Delta-Varint compression, Zero-Copy memory management, and adaptive kernels. Experimental results on Friendster (65.6M nodes, 3.6B edges) demonstrate that while traditional RCM degrades compression performance by 15-30\%, our Gap-Aware BFS achieves a \textbf{9.2\% speedup over the optimized baseline} (6.33s vs 6.91s) by preserving community structure during renumbering. This study demonstrates that preprocessing can resolve the antagonism between data locality and compression through careful design.
\end{abstract}

% ------------------------------------------------------------------------------
% Section 1: Introduction
% ------------------------------------------------------------------------------
\section{Introduction}

Graph preprocessing is a critical but often overlooked component of high-performance graph analytics. The physical layout of vertices and edges in memory significantly affects cache utilization, TLB efficiency, and memory bandwidth requirements---key factors that determine whether algorithms are compute-bound or memory-bound on modern GPUs.

\textbf{This project's primary contribution is a systematic study of graph preprocessing techniques, with emphasis on the evolution from RCM to Gap-Aware BFS, and their impact on GPU-accelerated graph algorithms.} While we implement two fundamental algorithms (BFS and Afforest) as case studies, the main objective is to demonstrate that \textbf{preprocessing choices are algorithm-dependent} and must be co-designed with runtime optimizations.

We evaluate several preprocessing and optimization strategies:
\begin{itemize}
    \item \textbf{Gap-Aware BFS}: A novel reordering strategy that sorts neighbors by original ID during layout construction to preserve compression-friendly deltas.
    \item \textbf{RCM Reordering}: Bandwidth minimization for improved spatial locality in uncompressed data.
    \item \textbf{Delta-Compressed CSR}: Varint encoding reducing data transfer by 37\%
    \item \textbf{Zero-Copy Memory}: Streaming access to graphs exceeding VRAM
    \item \textbf{Adaptive Kernels}: Dynamic load balancing for irregular graphs
\end{itemize}

Our experimental results reveal a key insight: \textbf{Standard RCM and compression are antagonistic}, but \textbf{Gap-Aware BFS resolves this trade-off}. While RCM improves cache locality for uncompressed algorithms, it scatters IDs and ruins compression. In contrast, Gap-Aware BFS achieves the spatial locality of RCM while maintaining the small deltas required for efficient Varint encoding.

% ------------------------------------------------------------------------------
% Section 2: Algorithm Design
% ------------------------------------------------------------------------------
\section{Algorithm Design}

\subsection{Graph Representation}

\paragraph{Standard CSR.}
The CSR format consists of two arrays: \texttt{row\_ptr[n+1]} defining adjacency list boundaries and \texttt{col\_idx[m]} storing neighbor indices. This representation is ideal for GPU parallelism as it allows coalesced memory access patterns.

\paragraph{Compressed CSR (c-CSR).}
To reduce memory bandwidth, we implement Delta+Varint compression:
\begin{enumerate}
    \item Sort neighbors per row: $[5, 12, 8, 20] \rightarrow [5, 8, 12, 20]$
    \item Compute deltas: $\Delta = [5, 3, 4, 8]$  
    \item Varint encode: Small deltas use 1 byte, large deltas use up to 5 bytes
\end{enumerate}

The \texttt{row\_ptr} array now stores \textbf{byte offsets} instead of edge indices, and \texttt{compressed\_col} is a byte stream requiring on-the-fly decoding.

\subsection{Adaptive BFS}

The Adaptive BFS kernel dynamically classifies frontier nodes based on degree:
\begin{itemize}
    \item \textbf{Small degree} ($<$ threshold): Process sequentially per thread
    \item \textbf{Large degree} ($\geq$ threshold): Cooperative warp processing
\end{itemize}

This classification reduces warp divergence and balances load across threads. The kernel uses shared memory for cooperative neighbor expansion and ballot sync for efficient atomic reduction.

\subsection{Compressed BFS}

The compressed BFS kernel employs a warp-cooperative strategy to decode Varint-encoded neighbors efficiently. Each warp (32 threads) processes one frontier node cooperatively. Lane 0 performs serial delta decoding: it reads the compressed byte stream, decodes each Varint to recover delta values, reconstructs absolute neighbor IDs by accumulating deltas, and stores results in shared memory. After synchronization via warp barrier, all 32 lanes process the decoded neighbors in parallel using a strided loop pattern (lane $i$ processes neighbors $i, i+32, i+64, \ldots$).

This design balances the inherently serial nature of delta decoding (each neighbor depends on the previous) with parallel neighbor processing. The use of shared memory (48KB per SM) allows fast broadcast of decoded data to all warp lanes. For high-degree nodes, the kernel processes neighbors in chunks of 32, maximizing occupancy while minimizing shared memory pressure.

\textbf{Varint Decoding Details:} Each integer is encoded as 1-5 bytes. The decoder reads bytes sequentially, extracting 7-bit payloads and checking the continuation bit (bit 7). Small deltas (common in sorted neighbor lists) use only 1 byte, while large deltas use up to 5 bytes. This variable-length encoding achieves 30-40\% compression on real-world graphs with power-law degree distributions.

\textbf{Bandwidth Optimization:} By streaming compressed data over PCIe, we reduce the effective bandwidth requirement from 14.4GB to 9.1GB per full graph traversal. The decoding overhead (7-10 cycles per Varint) is partially masked by PCIe latency ($\sim$400ns), making this approach highly effective for bandwidth-bound workloads.

\subsection{Afforest Algorithm}

Afforest is a GPU-optimized Union-Find algorithm for connected components:

\paragraph{Algorithm Phases.}
\begin{enumerate}
    \item \textbf{Init}: Each node is its own parent: $parent[i] = i$
    \item \textbf{Link}: For each edge $(u, v)$: $parent[\max(comp_u, comp_v)] = \min(comp_u, comp_v)$
    \item \textbf{Compress}: Path compression: $parent[i] = parent[parent[i]]$
\end{enumerate}

\paragraph{GCC Pruning Optimization.}
For graphs with a Giant Connected Component (GCC) containing the majority of nodes, we implement an early-exit optimization. Before processing each edge $(u,v)$, the kernel performs path compression on both endpoints to find their current component roots $comp_u$ and $comp_v$. If both roots equal the pre-identified GCC ID (heuristically set to $parent[0]$ after initialization), the edge is skipped entirely.

This optimization is particularly effective on real-world graphs like Friendster where $>99.9\%$ of nodes belong to a single giant component. By converting expensive atomic minimum operations (which require cache coherence traffic and potential retry loops) into cheap read-only comparisons, we reduce memory contention significantly. The pruning check adds negligible overhead (2 integer comparisons) while eliminating atomic writes for the vast majority of edges in later iterations.

\textbf{Performance Impact:} On Friendster, approximately 3.5 billion out of 3.6 billion edges connect nodes within the GCC. Without pruning, each edge triggers an atomic operation even when both endpoints are already merged. With pruning, these become no-ops, reducing atomic contention by $\sim$97\%. However, the bandwidth bottleneck (reading compressed edge data) remains, limiting the overall speedup to $\sim$9\% (21.6s vs 23.5s).

\paragraph{Single-Pass Strategy.}
Unlike traditional Afforest which requires convergence checking (2-3 passes), our optimized version runs a single pass, which is sufficient for graphs with small diameter. This reduces runtime from $\sim$46s to $\sim$21s on Friendster.

% ------------------------------------------------------------------------------
% Section 3: Memory Management
% ------------------------------------------------------------------------------
\section{Memory Management for Large Graphs}

\subsection{Zero-Copy Strategy}

For graphs exceeding GPU VRAM (Friendster: 13.5GB edge data on 5.8GB GPU), we employ CUDA's Zero-Copy mechanism with pinned host memory. The implementation follows a three-step process:

\textbf{Step 1: Memory Pinning.} We register the host-allocated edge array with \texttt{cudaHostRegister(..., cudaHostRegisterMapped)}, which locks the physical pages in RAM and creates a device-accessible mapping. This prevents the OS from swapping pages and enables direct GPU access via PCIe.

\textbf{Step 2: Device Pointer Acquisition.} Using \texttt{cudaHostGetDevicePointer()}, we obtain a GPU-side pointer to the pinned host memory. This pointer is valid in device code and transparently routes memory accesses through the PCIe bus.

\textbf{Step 3: Kernel Execution.} GPU kernels access the edge array as if it were in VRAM. The CUDA driver automatically handles PCIe transfers, fetching cache lines on-demand. Coalesced memory access patterns (threads in a warp accessing consecutive addresses) are crucial for achieving reasonable bandwidth.

\textbf{Memory Advise Hints:} We use \texttt{cudaMemAdvise(..., cudaMemAdviseSetReadMostly)} to inform the driver that the graph structure is read-only, enabling optimizations like caching in GPU L2 without write-back overhead. Additionally, \texttt{cudaMemPrefetchAsync()} can pre-load frequently accessed data (e.g., row pointers) into GPU memory before kernel launch.

\textbf{Performance Characteristics:}
\begin{itemize}
    \item PCIe 4.0 bandwidth: $\sim$16 GB/s theoretical, $\sim$12 GB/s effective
    \item Zero-copy latency: Higher than VRAM ($\sim$400ns vs $\sim$100ns)
    \item Bandwidth-bound: Performance limited by data transfer rate, not compute
\end{itemize}

Our streaming access patterns achieve $\sim$0.7 GB/s effective bandwidth on the RTX 3060 Laptop GPU.

\subsection{Compression Benefits}

Delta-Varint compression provides:
\begin{itemize}
    \item \textbf{Bandwidth Reduction}: 14.4 GB $\rightarrow$ 9.1 GB (37\% reduction)
    \item \textbf{Lower PCIe Traffic}: Fewer bytes transferred per traversal
    \item \textbf{Decoding Overhead}: Partially masked by memory latency
\end{itemize}

The net effect is a $\sim$9\% speedup for Afforest (21.6s vs 23.5s) and $\sim$2.6x for BFS (4.5s vs 12s).

% ------------------------------------------------------------------------------
% Section 4: Results
% ------------------------------------------------------------------------------
\section{Experimental Results}

\subsection{Test Environment}
\begin{itemize}
    \item \textbf{GPU}: NVIDIA GeForce RTX 3060 Laptop (Ampere, sm\_86)
    \item \textbf{VRAM}: 5.8 GB
    \item \textbf{System RAM}: 32 GB
    \item \textbf{CUDA}: Version 12.5
    \item \textbf{Compiler}: nvcc with -O3, -arch=sm\_86
\end{itemize}

\subsection{Benchmark Datasets}

\textbf{Friendster Social Network:}
\begin{itemize}
    \item Vertices: 65,608,366
    \item Edges: 3,612,134,270 (3.6 billion)
    \item Edge data size: 13.46 GB (exceeds VRAM)
    \item Max degree: 5,214
    \item Graph diameter: 22
    \item Compressed size: 9.13 GB (1.48x ratio)
\end{itemize}

\subsection{Performance Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Mode} & \textbf{Time (ms)} & \textbf{Effective Speedup} \\
\midrule
\textbf{Adaptive BFS} (Original) & Uncompressed & 12,009 & 1.0x \\
\textbf{Adaptive BFS} (Original) & \textbf{Compressed} & \textbf{6,908} & \textbf{1.74x} \\
\textbf{Adaptive BFS} (Gap-Aware) & \textbf{Compressed} & \textbf{6,334} & \textbf{1.90x} \\
\midrule
\textbf{Afforest} & Uncompressed & 23,512 & 1.0x \\
\textbf{Afforest} & \textbf{Compressed} & \textbf{21,569} & \textbf{1.09x} \\
\bottomrule
\end{tabular}
\caption{Final Performance on Friendster (100\% Reachability). Gap-Aware BFS provides the optimal balance. All runs use Zero-Copy memory.}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{BFS (Gap-Aware Compressed)} & \textbf{Afforest (Compressed)} \\
\midrule
Runtime & 6.33 s & 21.57 s \\
Memory (Host) & 9.13 GB & 9.13 GB \\
Effective Bandwidth & $\sim$1.44 GB/s & $\sim$0.42 GB/s \\
Correctness & 100\% (verified) & 1 component (correct) \\
\bottomrule
\end{tabular}
\caption{Detailed metrics for our best performing configurations on Friendster.}
\end{table}

\subsection{Analysis}

The 37\% data reduction translates to different speedups for BFS (1.9x) vs Afforest (1.09x). BFS benefits more because:
\begin{itemize}
    \item BFS performs a single pass with higher streaming density per frontier layer.
    \item Afforest's atomic operations (component merging) partially mask bandwidth gains.
    \item Gap-Aware BFS further improves L2 cache utilization by ~9\% over original crawl order.
\end{itemize}

\textbf{Zero-Copy Performance:}
Despite accessing 9-14 GB of data over PCIe, both algorithms achieve practical runtimes. The key is \textbf{sequential streaming}: coalesced reads from the GPU exploit PCIe burst transfers effectively.

\textbf{GCC Pruning Impact:}
For Afforest, the GCC pruning optimization provides marginal $(< 10\%)$ benefit on Friendster because:
\begin{itemize}
    \item Pruning converts atomic writes to reads, but both operations traverse the same edges
    \item The bandwidth bottleneck (reading compressed data) is not eliminated
    \item Primary benefit: Reduced atomic contention on highly connected components
\end{itemize}

\textbf{Single-Pass vs Multi-Pass:}
Using a single-pass Afforest reduces runtime from 46s (convergence loop) to 21.6s. This works because Friendster has diameter 22: one full edge scan connects $>99.9\%$ of the graph.

% ------------------------------------------------------------------------------
% Section 5: Graph Preprocessing: RCM and Gap-Aware BFS
% ------------------------------------------------------------------------------
\section{Graph Preprocessing: RCM and Gap-Aware BFS}

\subsection{Motivation and Algorithm}

The Reverse Cuthill-McKee (RCM) algorithm is a graph reordering technique that reduces the \textbf{bandwidth} of the adjacency matrix by renumbering vertices to place connected nodes close together in memory. This improves:
\begin{itemize}
    \item \textbf{Cache locality}: Adjacent vertices in BFS trees map to nearby memory addresses
    \item \textbf{TLB hit rate}: Reduced page working set due to spatial clustering
    \item \textbf{Prefetcher efficiency}: Sequential access patterns enable hardware prefetching
\end{itemize}

\paragraph{RCM Algorithm Overview.}
\begin{enumerate}
    \item Select a peripheral node (low degree, far from graph center) as the starting vertex
    \item Perform BFS traversal, visiting nodes level-by-level
    \item Within each level, sort nodes by \textbf{increasing degree} (Cuthill-McKee)
    \item \textbf{Reverse} the final ordering (Reverse Cuthill-McKee)
\end{enumerate}

The reversal step places high-degree hub nodes at the beginning of the ordering, which empirically improves performance for many graph algorithms by prioritizing well-connected vertices early in traversals.

\subsection{Implementation Details}

Our CUDA implementation uses:
\begin{itemize}
    \item \textbf{Parallel BFS}: GPU-based level-synchronous traversal for fast reordering
    \item \textbf{Radix Sort}: Per-level sorting by degree using CUB library primitives
    \item \textbf{Pseudo-Peripheral Search}: Heuristic to find optimal starting vertex
\end{itemize}

Reordering time for Friendster: $\sim$2.3 seconds for RCM, $\sim$18 minutes for Gap-Aware BFS (due to neighbor sorting).

\subsection{Gap-Aware BFS: Resolving the Locality-Compression Trade-off}

To address the performance degradation seen when combining RCM with compression, we implemented \textbf{Gap-Aware BFS}. The algorithm follows the standard BFS renumbering but introduces a critical modification:
\textbf{During the neighbor enqueue stage, neighbors are sorted by their ORIGINAL ID before being assigned NEW IDs.}

This modification ensures that nodes that were originally part of the same community (and thus had close IDs) are assigned consecutive new IDs. This preserves the "Natural Community Order" benefits for Delta-Varint compression while obtaining the bandwidth reduction benefits of a global BFS ordering.

\subsection{Experimental Results}

We compare four graph orderings on Friendster:
\begin{enumerate}
    \item \textbf{Original Order}: Natural edge insertion order from web crawling.
    \item \textbf{RCM Order}: Bandwidth-minimized via standard RCM.
    \item \textbf{Gap-Aware BFS}: Locality-optimized with original gap preservation.
    \item \textbf{Degree-Sorted}: Baseline reordering (not recommended).
\end{enumerate}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm \& Mode} & \textbf{Original} & \textbf{Gap-Aware BFS} & \textbf{Speedup / Delta} \\
\midrule
BFS Compressed & 6,908 ms & \textbf{6,334 ms} & -8.3\% (Best) \\
BFS Uncompressed & 12,009 ms & 8,812 ms & -26.6\% (Good) \\
RCM (Uncompressed) & 12,009 ms & 8,602 ms & -28.3\% (Peak Locality) \\
\bottomrule
\end{tabular}
\caption{Comparison of Reordering Strategies on Friendster (100\% Reachability). Gap-Aware BFS provides the best balance for compressed data.}
\end{table}

\subsection{Analysis: Resolving the Antagonism}

\paragraph{Locality Gains.}
Like RCM, Gap-Aware BFS provides a global leveling that ensures nodes visited together are stored together. This reduced our compressed runtime from 6.9s in the original crawl order to 6.3s.

\paragraph{Compression Efficiency.}
The "sorting-by-original-ID" trick was the breakthrough. By keeping the delta gaps small ($\Delta \approx 1$ for community members), we avoided the 15-30\% penalty observed with standard RCM. Gap-Aware BFS is the only reordering strategy we evaluated that successfully combines both optimization axes.

\subsection{Design Guidelines}

Based on our results, we recommend:
\begin{itemize}
    \item \textbf{Use Gap-Aware BFS} for: Optimal performance on compressed social graphs. It provides 8-10\% speedup over the crawl order without breaking compression.
    \item \textbf{Use RCM} for: Uncompressed, cache-sensitive algorithms where total bandwidth is not the bottleneck but latency is.
\end{itemize}

\textbf{Key Insight:} There is no "universal" optimal preprocessing. RCM optimizes the \emph{global matrix envelope} (bandwidth), while compression requires \emph{local neighbor proximity} (gaps). Ideally, one should use community-preserving reordering for compression, which remains an open area for future optimization in this system.

% ------------------------------------------------------------------------------
% Section 6: Conclusion
% ------------------------------------------------------------------------------
\section{Conclusion}

This project provides a comprehensive study of \textbf{graph preprocessing and its interaction with GPU optimization strategies}. The central contribution is demonstrating that preprocessing choices (RCM reordering vs. natural ordering) have \textbf{algorithm-dependent} performance impacts that must be considered during system design.

\paragraph{Key Findings.}
\begin{itemize}
    \item \textbf{RCM reordering provides 28\% speedup} for uncompressed BFS but severely degrades delta-compression efficiency.
    \item \textbf{Gap-Aware BFS resolves the locality-compression conflict}, achieving a 9.2\% speedup over the optimized baseline on Friendster.
    \item \textbf{Hierarchical locality preservation} is the key to managing billion-scale graphs on consumer GPUs: small gaps for compression and global leveling for cache efficiency.
\end{itemize}

\paragraph{Broader Impact.}
Beyond the specific algorithms implemented (BFS and Afforest), this work establishes \textbf{design principles for graph preprocessing}:
\begin{enumerate}
    \item Analyze algorithm memory access patterns (random vs. sequential, read-heavy vs. atomic-heavy)
    \item Measure sensitivity to cache locality vs. memory bandwidth
    \item Select preprocessing strategy that aligns with the dominant bottleneck
    \item Consider one-time preprocessing cost vs. repeated execution benefits
\end{enumerate}

Graph preprocessing is not a preprocessing step to be applied blindly---it is a \textbf{co-design problem} requiring deep understanding of both algorithm characteristics and hardware constraints. On memory-limited GPUs processing billion-scale graphs, the choice between RCM reordering and compression-friendly orderings can determine whether an algorithm is practical or infeasible.

Future work should explore adaptive preprocessing that selects orderings dynamically based on runtime profiling, as well as hybrid schemes that apply RCM to subgraphs while preserving global clustering properties.

\vfill
\noindent\textbf{Source Code:} \url{https://github.com/EvangelosMoschou/PkDSProject3.git}

\end{document}
