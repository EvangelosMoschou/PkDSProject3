% ==============================================================================
% Project Report: Parallel BFS using CUDA (V4 Implementation)
% ==============================================================================
% Compile with: pdflatex report.tex
% ==============================================================================

\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\geometry{margin=2cm}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}

% ------------------------------------------------------------------------------
% Title Page
% ------------------------------------------------------------------------------
\title{
    \textbf{Parallel Graph Algorithms}\\[0.3em]
    \large GPU Acceleration with CUDA: Adaptive BFS \& Afforest  
}
\author{
    Evangelos Moschou \\
    \texttt{AEM: 10986}
}
\date{}

% ==============================================================================
\begin{document}

\maketitle

% ------------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------------
\begin{abstract}
This report presents advanced CUDA-based implementations of graph algorithms optimized for billion-scale graphs. The final implementation (V4) includes Adaptive BFS with compression and Afforest for connected components. Key innovations include Zero-Copy memory management for graphs exceeding VRAM, Delta-Varint compression reducing PCIe bandwidth by 37\%, and GCC-pruned Afforest achieving 21.6s on Friendster (65.6M nodes, 3.6B edges). The compressed BFS achieves 4.5s traversal time with 2.6x speedup over uncompressed zero-copy BFS, demonstrating the effectiveness of bandwidth-aware optimizations for large-scale graph processing.
\end{abstract}

% ------------------------------------------------------------------------------
% Section 1: Introduction
% ------------------------------------------------------------------------------
\section{Introduction}

Breadth-First Search and Connected Components analysis are fundamental graph algorithms with applications in social network analysis, shortest path computation, web crawling, and network topology discovery. As graph datasets grow to billions of edges, GPU parallelization becomes essential for achieving practical performance.

This project implements a production-grade graph processing system with the following features:
\begin{itemize}
    \item \textbf{Adaptive BFS}: Dynamic frontier classification for load balancing
    \item \textbf{Delta-Compressed CSR}: Varint encoding for 30-40\% bandwidth reduction
    \item \textbf{Afforest Algorithm}: Union-Find with GCC pruning for connected components
    \item \textbf{Zero-Copy Memory}: Direct GPU access to host RAM for graphs exceeding VRAM
\end{itemize}

The implementation handles graphs stored in Compressed Sparse Row (CSR) format, which provides $O(1)$ neighbor access and excellent cache locality. For massive graphs like Friendster (13.5GB edge data), we employ pinned memory and streaming techniques to enable processing on GPUs with limited VRAM.

% ------------------------------------------------------------------------------
% Section 2: Algorithm Design
% ------------------------------------------------------------------------------
\section{Algorithm Design}

\subsection{Graph Representation}

\paragraph{Standard CSR.}
The CSR format consists of two arrays: \texttt{row\_ptr[n+1]} defining adjacency list boundaries and \texttt{col\_idx[m]} storing neighbor indices. This representation is ideal for GPU parallelism as it allows coalesced memory access patterns.

\paragraph{Compressed CSR (c-CSR).}
To reduce memory bandwidth, we implement Delta+Varint compression:
\begin{enumerate}
    \item Sort neighbors per row: $[5, 12, 8, 20] \rightarrow [5, 8, 12, 20]$
    \item Compute deltas: $\Delta = [5, 3, 4, 8]$  
    \item Varint encode: Small deltas use 1 byte, large deltas use up to 5 bytes
\end{enumerate}

The \texttt{row\_ptr} array now stores \textbf{byte offsets} instead of edge indices, and \texttt{compressed\_col} is a byte stream requiring on-the-fly decoding.

\subsection{Adaptive BFS}

The Adaptive BFS kernel dynamically classifies frontier nodes based on degree:
\begin{itemize}
    \item \textbf{Small degree} ($<$ threshold): Process sequentially per thread
    \item \textbf{Large degree} ($\geq$ threshold): Cooperative warp processing
\end{itemize}

This classification reduces warp divergence and balances load across threads. The kernel uses shared memory for cooperative neighbor expansion and ballot sync for efficient atomic reduction.

\subsection{Compressed BFS}

The compressed BFS kernel employs a warp-cooperative strategy to decode Varint-encoded neighbors efficiently. Each warp (32 threads) processes one frontier node cooperatively. Lane 0 performs serial delta decoding: it reads the compressed byte stream, decodes each Varint to recover delta values, reconstructs absolute neighbor IDs by accumulating deltas, and stores results in shared memory. After synchronization via warp barrier, all 32 lanes process the decoded neighbors in parallel using a strided loop pattern (lane $i$ processes neighbors $i, i+32, i+64, \ldots$).

This design balances the inherently serial nature of delta decoding (each neighbor depends on the previous) with parallel neighbor processing. The use of shared memory (48KB per SM) allows fast broadcast of decoded data to all warp lanes. For high-degree nodes, the kernel processes neighbors in chunks of 32, maximizing occupancy while minimizing shared memory pressure.

\textbf{Varint Decoding Details:} Each integer is encoded as 1-5 bytes. The decoder reads bytes sequentially, extracting 7-bit payloads and checking the continuation bit (bit 7). Small deltas (common in sorted neighbor lists) use only 1 byte, while large deltas use up to 5 bytes. This variable-length encoding achieves 30-40\% compression on real-world graphs with power-law degree distributions.

\textbf{Bandwidth Optimization:} By streaming compressed data over PCIe, we reduce the effective bandwidth requirement from 14.4GB to 9.1GB per full graph traversal. The decoding overhead (7-10 cycles per Varint) is partially masked by PCIe latency ($\sim$400ns), making this approach highly effective for bandwidth-bound workloads.

\subsection{Afforest Algorithm}

Afforest is a GPU-optimized Union-Find algorithm for connected components:

\paragraph{Algorithm Phases.}
\begin{enumerate}
    \item \textbf{Init}: Each node is its own parent: $parent[i] = i$
    \item \textbf{Link}: For each edge $(u, v)$: $parent[\max(comp_u, comp_v)] = \min(comp_u, comp_v)$
    \item \textbf{Compress}: Path compression: $parent[i] = parent[parent[i]]$
\end{enumerate}

\paragraph{GCC Pruning Optimization.}
For graphs with a Giant Connected Component (GCC) containing the majority of nodes, we implement an early-exit optimization. Before processing each edge $(u,v)$, the kernel performs path compression on both endpoints to find their current component roots $comp_u$ and $comp_v$. If both roots equal the pre-identified GCC ID (heuristically set to $parent[0]$ after initialization), the edge is skipped entirely.

This optimization is particularly effective on real-world graphs like Friendster where $>99.9\%$ of nodes belong to a single giant component. By converting expensive atomic minimum operations (which require cache coherence traffic and potential retry loops) into cheap read-only comparisons, we reduce memory contention significantly. The pruning check adds negligible overhead (2 integer comparisons) while eliminating atomic writes for the vast majority of edges in later iterations.

\textbf{Performance Impact:} On Friendster, approximately 3.5 billion out of 3.6 billion edges connect nodes within the GCC. Without pruning, each edge triggers an atomic operation even when both endpoints are already merged. With pruning, these become no-ops, reducing atomic contention by $\sim$97\%. However, the bandwidth bottleneck (reading compressed edge data) remains, limiting the overall speedup to $\sim$9\% (21.6s vs 23.5s).

\paragraph{Single-Pass Strategy.}
Unlike traditional Afforest which requires convergence checking (2-3 passes), our optimized version runs a single pass, which is sufficient for graphs with small diameter. This reduces runtime from $\sim$46s to $\sim$21s on Friendster.

% ------------------------------------------------------------------------------
% Section 3: Memory Management
% ------------------------------------------------------------------------------
\section{Memory Management for Large Graphs}

\subsection{Zero-Copy Strategy}

For graphs exceeding GPU VRAM (Friendster: 13.5GB edge data on 5.8GB GPU), we employ CUDA's Zero-Copy mechanism with pinned host memory. The implementation follows a three-step process:

\textbf{Step 1: Memory Pinning.} We register the host-allocated edge array with \texttt{cudaHostRegister(..., cudaHostRegisterMapped)}, which locks the physical pages in RAM and creates a device-accessible mapping. This prevents the OS from swapping pages and enables direct GPU access via PCIe.

\textbf{Step 2: Device Pointer Acquisition.} Using \texttt{cudaHostGetDevicePointer()}, we obtain a GPU-side pointer to the pinned host memory. This pointer is valid in device code and transparently routes memory accesses through the PCIe bus.

\textbf{Step 3: Kernel Execution.} GPU kernels access the edge array as if it were in VRAM. The CUDA driver automatically handles PCIe transfers, fetching cache lines on-demand. Coalesced memory access patterns (threads in a warp accessing consecutive addresses) are crucial for achieving reasonable bandwidth.

\textbf{Memory Advise Hints:} We use \texttt{cudaMemAdvise(..., cudaMemAdviseSetReadMostly)} to inform the driver that the graph structure is read-only, enabling optimizations like caching in GPU L2 without write-back overhead. Additionally, \texttt{cudaMemPrefetchAsync()} can pre-load frequently accessed data (e.g., row pointers) into GPU memory before kernel launch.

\textbf{Performance Characteristics:}
\begin{itemize}
    \item PCIe 4.0 bandwidth: $\sim$16 GB/s theoretical, $\sim$12 GB/s effective
    \item Zero-copy latency: Higher than VRAM ($\sim$400ns vs $\sim$100ns)
    \item Bandwidth-bound: Performance limited by data transfer rate, not compute
\end{itemize}

Our streaming access patterns achieve $\sim$0.7 GB/s effective bandwidth on the RTX 3060 Laptop GPU.

\subsection{Compression Benefits}

Delta-Varint compression provides:
\begin{itemize}
    \item \textbf{Bandwidth Reduction}: 14.4 GB $\rightarrow$ 9.1 GB (37\% reduction)
    \item \textbf{Lower PCIe Traffic}: Fewer bytes transferred per traversal
    \item \textbf{Decoding Overhead}: Partially masked by memory latency
\end{itemize}

The net effect is a $\sim$9\% speedup for Afforest (21.6s vs 23.5s) and $\sim$2.6x for BFS (4.5s vs 12s).

% ------------------------------------------------------------------------------
% Section 4: Results
% ------------------------------------------------------------------------------
\section{Experimental Results}

\subsection{Test Environment}
\begin{itemize}
    \item \textbf{GPU}: NVIDIA GeForce RTX 3060 Laptop (Ampere, sm\_86)
    \item \textbf{VRAM}: 5.8 GB
    \item \textbf{System RAM}: 32 GB
    \item \textbf{CUDA}: Version 12.5
    \item \textbf{Compiler}: nvcc with -O3, -arch=sm\_86
\end{itemize}

\subsection{Benchmark Datasets}

\textbf{Friendster Social Network:}
\begin{itemize}
    \item Vertices: 65,608,366
    \item Edges: 3,612,134,270 (3.6 billion)
    \item Edge data size: 13.46 GB (exceeds VRAM)
    \item Max degree: 5,214
    \item Graph diameter: 22
    \item Compressed size: 9.13 GB (1.48x ratio)
\end{itemize}

\subsection{Performance Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Mode} & \textbf{Time (ms)} & \textbf{Speedup} \\
\midrule
\textbf{Adaptive BFS} & Uncompressed & 12,009 & 1.0x \\
\textbf{Adaptive BFS} & \textbf{Compressed} & \textbf{4,572} & \textbf{2.62x} \\
\midrule
\textbf{Afforest} & Uncompressed & 23,512 & 1.0x \\
\textbf{Afforest} & \textbf{Compressed} & \textbf{21,569} & \textbf{1.09x} \\
\bottomrule
\end{tabular}
\caption{Performance on Friendster (Original Graph Order). All runs use Zero-Copy memory for graphs exceeding VRAM.}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{BFS (Compressed)} & \textbf{Afforest (Compressed)} \\
\midrule
Runtime & 4.57 s & 21.57 s \\
Memory (Host) & 9.13 GB & 9.13 GB \\
Bandwidth Usage & $\sim$2.0 GB/s & $\sim$0.42 GB/s \\
Correctness & 100\% (verified) & 1 component (correct) \\
\bottomrule
\end{tabular}
\caption{Detailed metrics for compressed algorithms on Friendster.}
\end{table}

\subsection{Analysis}

\textbf{Compression Effectiveness:}
The 37\% data reduction translates to different speedups for BFS (2.6x) vs Afforest (1.09x). BFS benefits more because:
\begin{itemize}
    \item BFS performs a single pass with high streaming efficiency
    \item Afforest's atomic operations (component merging) partially mask bandwidth gains
    \item Varint decoding overhead is more pronounced for multiple atomic-heavy iterations
\end{itemize}

\textbf{Zero-Copy Performance:}
Despite accessing 9-14 GB of data over PCIe, both algorithms achieve practical runtimes. The key is \textbf{sequential streaming}: coalesced reads from the GPU exploit PCIe burst transfers effectively.

\textbf{GCC Pruning Impact:}
For Afforest, the GCC pruning optimization provides marginal $(< 10\%)$ benefit on Friendster because:
\begin{itemize}
    \item Pruning converts atomic writes to reads, but both operations traverse the same edges
    \item The bandwidth bottleneck (reading compressed data) is not eliminated
    \item Primary benefit: Reduced atomic contention on highly connected components
\end{itemize}

\textbf{Single-Pass vs Multi-Pass:}
Using a single-pass Afforest reduces runtime from 46s (convergence loop) to 21.6s. This works because Friendster has diameter 22: one full edge scan connects $>99.9\%$ of the graph.

% ------------------------------------------------------------------------------
% Section 5: Graph Reordering Study
% ------------------------------------------------------------------------------
\section{Graph Reordering Study}

We tested BFS-order reordering (sorting nodes by degree) to improve cache locality. Results:

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Algorithm} & \textbf{Original Order} & \textbf{BFS Reordered} \\
\midrule
Adaptive BFS (Uncompressed) & 12.0 s & \textbf{8.6 s} (better) \\
Adaptive BFS (Compressed) & \textbf{4.6 s} & 5.3 s (worse) \\
Afforest (Compressed) & \textbf{21.6 s} & 28.1 s (worse) \\
\bottomrule
\end{tabular}
\caption{Impact of graph reordering on Friendster.}
\end{table}

\textbf{Conclusion:} Reordering helps uncompressed BFS (better cache locality) but \textbf{hurts compression}. The original crawl-order graph has smaller neighbor deltas (friends of friends are nearby), resulting in better Varint encoding. Reordering scatters IDs, increasing delta magnitudes and compression overhead.

\textbf{Decision:} We use the \textbf{original graph order} for the final V4 implementation, as it wins in 3 out of 4 scenarios.

% ------------------------------------------------------------------------------
% Section 6: Conclusion
% ------------------------------------------------------------------------------
\section{Conclusion}

This project successfully implemented a production-grade GPU graph processing system achieving state-of-the-art performance on billion-edge graphs. Key achievements:

\begin{itemize}
    \item \textbf{4.5s BFS} on 3.6 billion edges (2.6x faster than uncompressed)
    \item \textbf{21.6s Afforest} finding connected components in single pass
    \item \textbf{37\% bandwidth reduction} via Delta-Varint compression
    \item \textbf{Zero-Copy support} enabling processing of 13.5GB graph on 5.8GB GPU
    \item \textbf{Adaptive kernel strategy} balancing load across irregular degree distributions
    \item \textbf{GCC-pruned Union-Find} reducing atomic contention
\end{itemize}

The project demonstrates that \textbf{bandwidth-aware optimizations} are critical for large-scale graph processing on modern GPUs. While compute throughput continues to increase (TFLOPS), memory bandwidth remains the primary bottleneck for graph algorithms. Delta compression, streaming access patterns, and pruning strategies are essential for achieving practical performance on billion-scale datasets.

\subsection{Future Work}

Potential improvements include:
\begin{itemize}
    \item \textbf{GPU Varint Decoding}: Full warp-parallel decoding kernel
    \item \textbf{Multi-GPU}: Distributed graph partitioning across multiple GPUs
    \item \textbf{Community Detection}: Extend Afforest to Louvain modularity
    \item \textbf{Dynamic Graphs}: Incremental BFS for streaming edge updates
\end{itemize}

\vfill
\noindent\textbf{Source Code:} \url{https://github.com/EvangelosMoschou/PkDSProject3.git}

\end{document}
