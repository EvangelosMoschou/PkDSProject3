% ==============================================================================
% Project Report: High-Performance BFS on Billion-Scale Graphs (V5.3)
% ==============================================================================
% Compile with: pdflatex report.tex
% ==============================================================================

\documentclass[11pt,a4paper]{article}

% ------------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{left=1.5cm,right=1.5cm,top=2cm,bottom=2cm}
\usepackage{enumitem}
\setlist{nosep}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath}

% ------------------------------------------------------------------------------
% Title
% ------------------------------------------------------------------------------
\title{
    \textbf{High-Performance BFS on Billion-Scale Graphs}\\[0.3em]
    \large V3.3: 831ms on Friendster Using CUDA
}
\author{
    Evangelos Moschou \\
    \texttt{AEM: 10986}
}
\date{January 2026}

% ==============================================================================
\begin{document}

\maketitle

% ------------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------------
\begin{abstract}
This report presents the design and optimization of a GPU-accelerated Breadth-First Search (BFS) solver capable of traversing the Friendster social network (65.6M nodes, 3.6B edges) in \textbf{831 milliseconds}---a \textbf{26x speedup} over sequential CPU. For graphs that fit entirely in VRAM (20M nodes), we achieve an even higher \textbf{27.6x speedup} (190ms). We implement the three required versions: (1) \textbf{V1: Dynamic Thread Assignment}, (2) \textbf{V2: Chunk-Based Processing}, and (3) \textbf{V3: Shared-Memory Cooperative}. We further optimize V3 into \textbf{V3.3 (Final)}, introducing: (A) \textbf{Direct Queue Emission} to eliminate O(N) scans; (B) \textbf{Hybrid Direction-Optimization}; (C) \textbf{Delta-Varint Compression}; and (D) \textbf{Zero-Copy Streaming}.
\end{abstract}

% ------------------------------------------------------------------------------
% Section 1: Introduction
% ------------------------------------------------------------------------------
\section{Introduction}

Graph algorithms are fundamental to social network analysis, recommendation systems, and scientific computing. However, billion-scale graphs present significant challenges for GPU acceleration due to:
\begin{itemize}
    \item \textbf{Memory Constraints}: Large graphs exceed GPU VRAM (Friendster: 14.4GB vs 5.8GB available).
    \item \textbf{Irregular Access Patterns}: Power-law degree distributions cause load imbalance.
    \item \textbf{Atomic Contention}: Concurrent frontier updates create serialization bottlenecks.
\end{itemize}

This project addresses these challenges through a systematic optimization of the BFS algorithm, culminating in \textbf{Version 3.3} which achieves sub-second traversal on Friendster.

\subsection{Contributions}
\begin{enumerate}
    \item \textbf{Direct Queue Emission}: A novel technique where Bottom-Up kernels build the next frontier queue directly during traversal, eliminating expensive O(N) scans.
    \item \textbf{Warp-Aggregated Atomics}: Reducing global atomic contention by 32x through hierarchical aggregation.
    \item \textbf{Zero-Copy Streaming}: Enabling processing of out-of-core graphs via pinned host memory.
    \item \textbf{Comprehensive Evaluation}: Detailed performance analysis across algorithm versions.
\end{enumerate}

% ------------------------------------------------------------------------------
% Section 2: System Design
% ------------------------------------------------------------------------------
\section{System Design}

\subsection{Data Representation: Delta-Compressed CSR}

Standard CSR (Compressed Sparse Row) format stores:
\begin{itemize}
    \item \texttt{row\_ptr[N+1]}: Offsets into column array (8 bytes each for 64-bit indexing)
    \item \texttt{col\_idx[E]}: Neighbor IDs (4 bytes each)
\end{itemize}

For Friendster, this requires 14.4GB---exceeding our 5.8GB VRAM. We implement \textbf{Delta-Varint Compression}:
\begin{enumerate}
    \item Sort neighbors for each row
    \item Store differences: $\Delta_i = \text{neighbor}_i - \text{neighbor}_{i-1}$
    \item Encode deltas using Variable-Length Quantity (1-5 bytes per delta)
\end{enumerate}

This reduces Friendster to \textbf{9.13 GB} (37\% reduction), enabling Zero-Copy streaming from host RAM.

\subsection{Zero-Copy Memory Management}

Since compressed data still exceeds VRAM, we use \texttt{cudaHostRegister} to pin host memory and map it to device address space. Kernels access graph data over PCIe with effective bandwidth of $\sim$1.4 GB/s. The key insight is that BFS traversal exhibits high temporal locality per level---we stream each portion of the graph once per BFS iteration.

\subsection{Hybrid Direction-Optimization}

We implement the Beamer direction-optimization strategy:
\begin{itemize}
    \item \textbf{Top-Down}: Process small frontiers by expanding each frontier node's neighbors.
    \item \textbf{Bottom-Up}: Process large frontiers by checking if each unvisited node has a frontier neighbor.
\end{itemize}

The switching threshold is $N/26$ ($\approx$2.5M nodes for Friendster). This hybrid approach reduces redundant edge checks by up to 50\% on high-diameter graphs.

% ------------------------------------------------------------------------------
% Section 3: V3.3 Optimizations
% ------------------------------------------------------------------------------
\section{V3.3 Optimizations (Advanced V3)}

\subsection{Direct Queue Emission (Critical)}

In prior versions, Bottom-Up traversal marked newly discovered nodes by writing distances, then required a separate O(N) scan (\texttt{distancesToQueueKernel}) to rebuild the frontier queue. This scan dominated runtime on large graphs.

\textbf{V3.3 Solution}: Bottom-Up kernels now emit discovered nodes directly to the next frontier using warp-aggregated atomics:
\begin{enumerate}
    \item Each warp processes one unvisited node
    \item If a frontier neighbor is found, thread sets \texttt{found = true}
    \item Warp ballot collects all \texttt{found} flags: \texttt{\_\_ballot\_sync()}
    \item Leader thread atomically reserves space: \texttt{atomicAdd(queue\_size, popcount)}
    \item Each found thread writes its node ID at the computed offset
\end{enumerate}

This eliminates the O(N) scan entirely, reducing Bottom-Up level time by 100--150ms.

\subsection{Warp-Aggregated Atomics (Top-Down)}

Top-Down kernels originally used per-discovery \texttt{atomicAdd} calls to append to the next frontier. With billions of edges, this created severe contention.

\textbf{V3.3 Solution}: Within each warp, discoveries are aggregated before a single atomic operation:
\begin{enumerate}
    \item Each thread processes edges and flags discoveries
    \item \texttt{\_\_ballot\_sync()} + \texttt{\_\_popc()} count discoveries per warp
    \item Leader performs one \texttt{atomicAdd} for the entire warp
    \item \texttt{\_\_shfl\_sync()} broadcasts base index to all threads
    \item Each thread computes its local offset and writes
\end{enumerate}

This reduces global atomics from O(edges) to O(warps), a 32x reduction.

\subsection{Synchronization Reduction}

We removed unnecessary \texttt{cudaDeviceSynchronize()} calls after kernel launches, relying instead on CUDA's implicit stream serialization. This reduced per-level overhead by $\sim$10\%.

% ------------------------------------------------------------------------------
% Section 4: Experimental Results
% ------------------------------------------------------------------------------
\section{Experimental Results}

\subsection{Test Environment}
\begin{itemize}
    \item \textbf{GPU}: NVIDIA GeForce RTX 3060 Laptop (Ampere, sm\_86, 5.8GB VRAM)
    \item \textbf{System}: 32GB RAM, CUDA 12.5, Ubuntu Linux
    \item \textbf{Compiler}: nvcc -O3 -arch=sm\_86
\end{itemize}

\subsection{Benchmark: Friendster Social Network}
\begin{itemize}
    \item Vertices: 65,608,366
    \item Edges: 3,612,134,270 (3.6 billion)
    \item Uncompressed size: 14.4 GB
    \item Compressed size: 9.13 GB (1.58x ratio)
    \item Diameter: 22 levels
\end{itemize}

\subsection{Performance Evolution}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Version} & \textbf{Key Optimization} & \textbf{Time (ms)} & \textbf{Speedup} \\
\midrule
Sequential CPU & STL Queue + Vector & 21,708 & 1.0x \\
V1 Dynamic & Atomic Work Queue & 21,245 & 1.02x \\
V2 Chunked & Thread-Per-Chunk & 18,506 & 1.2x \\
V3.1 Shared & Shared Memory Kernels & 12,009 & 1.8x \\
V3.2 Hybrid & Direction-Optimization & 1,200 & 18.1x \\
\textbf{V3.3 Final} & \textbf{Direct Queue + Compression} & \textbf{831} & \textbf{26.1x} \\
\bottomrule
\end{tabular}
\caption{BFS Performance Evolution on Friendster (source=0)}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{BFS V3.3} & \textbf{Afforest} \\
\midrule
Runtime & \textbf{831 ms} & 21.6 s \\
Edges Processed & 3.6 B & 3.6 B \\
Throughput (GTEPS) & 4.35 & 0.17 \\
Memory Streamed & 9.13 GB & 9.13 GB \\
Effective Bandwidth & 11.0 GB/s & 0.42 GB/s \\
\bottomrule
\end{tabular}
\caption{Detailed metrics. GTEPS = Giga Traversed Edges Per Second.}
\end{table}

\subsection{Analysis}

The 26.1x speedup over CPU is achieved through:
\begin{itemize}
    \item \textbf{V3.1 Shared}: 1.8x speedup using shared memory, but limited by global atomics.
    \item \textbf{V3.2 Hybrid}: 10x jump by skipping redundant checks (Direction-Optimization).
    \item \textbf{V3.3 Final}: Further 30\% improvement via Compression and Direct Queue Emission.
\end{itemize}

It is important to note that the absolute performance (and thus the speedup) on Friendster is strictly bounded by the PCIe bus bandwidth. The compressed graph occupies \textbf{9.13 GB}, which exceeds the GPU's \textbf{5.8 GB VRAM}. Consequently, the GPU must stream data from system RAM during traversal.

\subsection{VRAM-Resident Graph: The Diameter Bottleneck}
To quantify algorithmic speedup without the PCIe bottleneck, we benchmarked a synthetic random graph (40M nodes, 400M edges, 2.5GB) that fits entirely in VRAM. However, theoretical "Uniform Random" generation on sparse graphs (Avg Degree 10) creates a "stringy" topology with a massive diameter (36,710), presenting a \textbf{pathological case} for massive parallelism.

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Version} & \textbf{Time (ms)} & \textbf{Diameter} & \textbf{Observation} \\
\midrule
Sequential CPU & \textbf{825} & 36,710 & \textbf{Fastest (Low Latency)} \\
V1 Dynamic & 1,265 & 36,710 & Kernel Launch Overhead \\
V3.1 Shared & 839 & 36,710 & Matches CPU \\
V3.3 Compressed & 4,101 & 36,710 & High Overhead per Level \\
\bottomrule
\end{tabular}
\caption{Performance on High-Diameter Sparse Graph (40M nodes)}
\end{table}

This result provides a crucial counter-intuitive insight: \textbf{GPUs require parallelism, not just bandwidth}.
\begin{itemize}
    \item \textbf{Kernel Overhead}: With a diameter of 36,710, the GPU must launch over 36,000 kernels. At $\approx$10$\mu$s per launch, the pure overhead is $\approx$360ms, nearly 40\% of the runtime.
    \item \textbf{Sequentiality}: The "frontier" at each level is small (often just a few nodes), causing most of the 1024 threads in a block to idle.
    \item \textbf{CPU Advantage}: The CPU processes this sequential chain efficiently in L1/L2 cache without synchronization overhead, actually beating V1 and V3.3.
\end{itemize}

This contrasts sharply with \textbf{Friendster} (Diameter $\approx$14), where the frontier contains millions of nodes, allowing the GPU to saturate 3500+ cores and achieve a \textbf{26x speedup}. Thus, our solvers are optimized for \textbf{Massive, Low-Diameter (Small World) Networks}, which represent real-world social graphs.

Afforest remains bandwidth-bound due to its random atomic linking pattern.

% ------------------------------------------------------------------------------
% Section 5: Afforest (Connected Components)
% ------------------------------------------------------------------------------
\section{Afforest Algorithm}

We implement the Afforest algorithm for connected components:
\begin{enumerate}
    \item \textbf{Sampling Phase} (Optional): Sample random edges to quickly link large components.
    \item \textbf{Hook Phase}: Iterate over all edges, linking components via \texttt{atomicMin}.
    \item \textbf{Compression}: Flatten component tree to root pointer.
\end{enumerate}

For compressed graphs, we disable sampling and GCC pruning (as component IDs cannot be reliably estimated). Runtime: 21.6s---primarily bandwidth-bound by the random atomic pattern.

% ------------------------------------------------------------------------------
% Section 6: Conclusion
% ------------------------------------------------------------------------------
\section{Conclusion}

This project demonstrates that billion-scale graph traversal is achievable in sub-second time on consumer GPUs through careful algorithmic optimization:
\begin{itemize}
    \item \textbf{831ms BFS on Friendster}: A 26x speedup over sequential CPU, enabled by Direct Queue Emission.
    \item \textbf{Warp-Level Aggregation}: Reduces atomic contention by 32x.
    \item \textbf{Zero-Copy + Compression}: Enables out-of-core processing with 37\% bandwidth reduction.
\end{itemize}

The key insight is that \textbf{algorithmic complexity matters more than raw bandwidth}. Eliminating the O(N) distance scan in V3.3 provided a larger improvement than all previous compression optimizations combined.

\vfill
\noindent\textbf{Source Code:} \url{https://github.com/EvangelosMoschou/PkDSProject3.git}

\end{document}
